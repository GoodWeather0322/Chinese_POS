{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, BertTokenizer\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# albert_tokenizer = AutoTokenizer.from_pretrained('voidful/albert_chinese_large')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"voidful/albert_chinese_large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tag_file = \"pos.tgt.dict\"\n",
    "ner_tag2id = {}\n",
    "\n",
    "count = 0\n",
    "with open(ner_tag_file) as fp:\n",
    "    for line in fp:\n",
    "        line = line.strip().split()\n",
    "        ner_tag2id[line[0]] = count\n",
    "        count += 1\n",
    "        \n",
    "ner_id2tag = {v:k for k, v in ner_tag2id.items()}\n",
    "        \n",
    "print(len(ner_tag2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag2id = {}\n",
    "\n",
    "count = 0\n",
    "for k, v in ner_tag2id.items():\n",
    "    if k.split('-')[-1] not in pos_tag2id:\n",
    "        pos_tag2id[k.split('-')[-1]] = count\n",
    "        count += 1\n",
    "        \n",
    "print(len(pos_tag2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"crawler_data/word_seg/data/cna_total_token_input\"\n",
    "label_file = \"crawler_data/word_seg/data/cna_total_token_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_token_input = []\n",
    "total_token_label = []\n",
    "\n",
    "with open(input_file) as fp1, open(label_file) as fp2:\n",
    "    for i, (tokens, labels) in enumerate(zip(fp1, fp2)):\n",
    "        tokens = tokens.strip().split()\n",
    "        labels = labels.strip().split()\n",
    "        \n",
    "        assert len(tokens) == len(labels)\n",
    "        \n",
    "        total_token_input.append(tokens)\n",
    "        total_token_label.append(labels)\n",
    "        \n",
    "print(len(total_token_input), len(total_token_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    \"\"\" Access dictionary keys like attribute \n",
    "        https://stackoverflow.com/questions/4984647/accessing-dict-keys-like-an-attribute\n",
    "    \"\"\"\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self\n",
    "\n",
    "opts = AttrDict()\n",
    "\n",
    "# Configure models\n",
    "opts.vocab_size = tokenizer.vocab_size\n",
    "opts.output_size = len(ner_tag2id)\n",
    "\n",
    "# Configure optimization\n",
    "opts.learning_rate = 1.5e-4\n",
    "opts.bert_lr = 5e-6\n",
    "opts.weight_decay = 0.01 # L2 weight regularization\n",
    "opts.max_grad_norm = 1.0\n",
    "\n",
    "opts.batch_size = 20\n",
    "\n",
    "# Configure training\n",
    "opts.max_seq_len = 256\n",
    "opts.num_epochs = 20\n",
    "opts.warmup_steps = 4000\n",
    "opts.gradient_accumulation = 6\n",
    "\n",
    "opts.load_pretrain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_seed = 202001004\n",
    "\n",
    "train_token, dev_token, train_label, dev_label = train_test_split(total_token_input, total_token_label, test_size=0.05, random_state=random_seed, shuffle=True)\n",
    "\n",
    "print(len(train_token), len(train_label), len(dev_token), len(dev_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(dev_token, dev_label):\n",
    "    assert len(a) == len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, token_input, token_label):\n",
    "        \n",
    "        self.token_inputs = token_input\n",
    "        self.token_labels = token_label\n",
    "        \n",
    "        print('total examples {} ...'.format(len(self.token_inputs)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        token_input = self.token_inputs[index]\n",
    "        token_label = self.token_labels[index]\n",
    "        \n",
    "        token_input = ['[CLS]'] + token_input + ['[SEP]']\n",
    "        token_label = ['[CLS]'] + token_label + ['[SEP]']\n",
    "        \n",
    "        input_ids = self.token2ids(token_input)\n",
    "        label_ids = self.label2ids(token_label)\n",
    "        \n",
    "        return token_input, token_label, input_ids, label_ids\n",
    "    \n",
    "    def token2ids(self, token_input):\n",
    "\n",
    "        return tokenizer.convert_tokens_to_ids(token_input)\n",
    "        \n",
    "    def label2ids(self, token_label):\n",
    "        \n",
    "        label_ids = [ner_tag2id[label] for label in token_label]\n",
    "        \n",
    "        return label_ids\n",
    "        \n",
    "        \n",
    "def collate_fn(data):\n",
    "    \n",
    "    inputs, labels, inputs_ids, labels_ids = zip(*data)    \n",
    "\n",
    "    \n",
    "    lens = [len(input_token) if len(input_token) < opts.max_seq_len else opts.max_seq_len for input_token in inputs]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    input_seqs = torch.zeros(len(inputs), max_len).long()\n",
    "    input_mask = torch.zeros(len(inputs), max_len)\n",
    "    targets = torch.zeros(len(inputs), max_len).long()\n",
    "    \n",
    "    for i, (input_ids, label_ids) in enumerate(zip(inputs_ids, labels_ids)):\n",
    "        input_seqs[i, :lens[i]] = torch.LongTensor(input_ids[:lens[i]])\n",
    "        input_mask[i, :lens[i]] = torch.ones(lens[i])\n",
    "        targets[i, :lens[i]] = torch.LongTensor(label_ids[:lens[i]])\n",
    "    \n",
    "    return inputs, labels, inputs_ids, labels_ids, input_seqs, input_mask, targets, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_token, train_label)\n",
    "dev_dataset = Dataset(dev_token, dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "train_iter = DataLoader(dataset=train_dataset,\n",
    "                        batch_size=opts.batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=8,\n",
    "#                         sampler=train_sampler,\n",
    "                        collate_fn=collate_fn)\n",
    "\n",
    "dev_iter = DataLoader(dataset=dev_dataset,\n",
    "                        batch_size=8,\n",
    "                        shuffle=False,\n",
    "                        num_workers=8,\n",
    "#                         sampler=train_sampler,\n",
    "                        collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Mapping\n",
    "\n",
    "class CRF(nn.Module):\n",
    "    \"\"\"Conditional random field.\n",
    "    This module implements a conditional random field [LMP]. The forward computation\n",
    "    of this class computes the log likelihood of the given sequence of tags and\n",
    "    emission score tensor. This class also has ``decode`` method which finds the\n",
    "    best tag sequence given an emission score tensor using `Viterbi algorithm`_.\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_tags : int\n",
    "        Number of tags.\n",
    "    batch_first : bool, optional\n",
    "        Whether the first dimension corresponds to the size of a minibatch.\n",
    "    Attributes\n",
    "    ----------\n",
    "    start_transitions : :class:`~torch.nn.Parameter`\n",
    "        Start transition score tensor of size ``(num_tags,)``.\n",
    "    end_transitions : :class:`~torch.nn.Parameter`\n",
    "        End transition score tensor of size ``(num_tags,)``.\n",
    "    transitions : :class:`~torch.nn.Parameter`\n",
    "        Transition score tensor of size ``(num_tags, num_tags)``.\n",
    "    References\n",
    "    ----------\n",
    "    .. [LMP] Lafferty, J., McCallum, A., Pereira, F. (2001).\n",
    "             \"Conditional random fields: Probabilistic models for segmenting and\n",
    "             labeling sequence data\". *Proc. 18th International Conf. on Machine\n",
    "             Learning*. Morgan Kaufmann. pp. 282â€“289.\n",
    "    .. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_tags: int, batch_first: bool = False, tag_id_to_name: Mapping[int, str] = None) -> None:\n",
    "        if num_tags <= 0:\n",
    "            raise ValueError(f'invalid number of tags: {num_tags}')\n",
    "        super().__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.batch_first = batch_first\n",
    "        self.start_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "        self.end_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "        self.transitions = nn.Parameter(torch.empty(num_tags, num_tags))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        if not tag_id_to_name: return\n",
    "\n",
    "        self.tag_id_to_name = dict()\n",
    "        for t_i_n in tag_id_to_name.items():\n",
    "            t_i, t_name = t_i_n\n",
    "            t_name_part = t_name.split('-')\n",
    "            assert len(t_name_part) <= 3, \"tag name {} error\".format(t_name)\n",
    "            t_pref = t_name_part[0]\n",
    "            t_suff = t_name_part[1] if len(t_name_part) > 1 else t_name_part[0]\n",
    "            if len(t_name_part) == 3:\n",
    "                t_suff += '-'\n",
    "                t_suff += t_name_part[2]\n",
    "            self.tag_id_to_name[t_i] = (t_pref, t_suff)\n",
    "\n",
    "        self.set_transition_constraint()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Initialize the transition parameters.\n",
    "        The parameters will be initialized randomly from a uniform distribution\n",
    "        between -0.01 and 0.01.\n",
    "        \"\"\"\n",
    "        nn.init.uniform_(self.start_transitions, -0.01, 0.01)\n",
    "        nn.init.uniform_(self.end_transitions, -0.01, 0.01)\n",
    "        nn.init.uniform_(self.transitions, -0.01, 0.01)\n",
    "\n",
    "    def set_transition_constraint(self) -> None:\n",
    "        \"\"\"Set impossible transitions\n",
    "        Set transitions between impossible labels to a very low score, effectively disabling them\n",
    "        \"\"\"\n",
    "        if len(self.tag_id_to_name) < 1:\n",
    "            print(\"no tag id to name dict\")\n",
    "            return\n",
    "        for source_i, (source_pref, source_suff) in self.tag_id_to_name.items():\n",
    "            if source_pref in ['[PAD]', '[SEP]']:\n",
    "                self.start_transitions.data[source_i] = -10000.\n",
    "                # can only go to PAD\n",
    "                self.transitions.data[source_i].fill_(-10000.)\n",
    "                for targ_i in range(len(self.tag_id_to_name)):\n",
    "                    targ_pref, targ_suff = self.tag_id_to_name[targ_i]\n",
    "                    if (targ_pref in ['[PAD]']):\n",
    "                        self.transitions.data[source_i, targ_i] = 0.001\n",
    "            if source_pref in ['[CLS]']:\n",
    "                # possible ends are S- and B-\n",
    "                self.transitions.data[source_i].fill_(-10000.)\n",
    "                for targ_i in range(len(self.tag_id_to_name)):\n",
    "                    targ_pref, targ_suff = self.tag_id_to_name[targ_i]\n",
    "                    if targ_pref in ['S', 'B']:\n",
    "                        self.transitions.data[source_i, targ_i] = 0.001\n",
    "                    if targ_pref in ['I', 'E']:\n",
    "                        self.transitions.data[source_i, targ_i] = -10000.\n",
    "            if source_pref in ['B', 'I']:\n",
    "                # possible ends are I- and E-\n",
    "                self.transitions.data[source_i].fill_(-10000.)\n",
    "                for targ_i in range(len(self.tag_id_to_name)):\n",
    "                    targ_pref, targ_suff = self.tag_id_to_name[targ_i]\n",
    "                    if (targ_suff == source_suff) and (targ_pref in ['I', 'E']):\n",
    "                        self.transitions.data[source_i, targ_i] = 0.001\n",
    "            if source_pref in ['S', 'E']:\n",
    "                # cannot go to I or E\n",
    "                for targ_i in range(len(self.tag_id_to_name)):\n",
    "                    targ_pref, targ_suff = self.tag_id_to_name[targ_i]\n",
    "                    if targ_pref in ['I', 'E']:\n",
    "                        self.transitions.data[source_i, targ_i] = -10000.\n",
    "            if source_pref in ['I', 'E']:\n",
    "                # cannot be start transitions\n",
    "                self.start_transitions.data[source_i] = -10000.\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(num_tags={self.num_tags})'\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            emissions: torch.Tensor,\n",
    "            tags: torch.LongTensor,\n",
    "            mask: Optional[torch.ByteTensor] = None,\n",
    "            reduction: str = 'sum',\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the conditional log likelihood of a sequence of tags given emission scores.\n",
    "        Arguments\n",
    "        ---------\n",
    "        emissions : :class:`~torch.Tensor`\n",
    "            Emission score tensor of size ``(seq_length, batch_size, num_tags)`` if\n",
    "            ``batch_first`` is ``False``, ``(batch_size, seq_length, num_tags)`` otherwise.\n",
    "        tags : :class:`~torch.LongTensor`\n",
    "            Sequence of tags tensor of size ``(seq_length, batch_size)`` if\n",
    "            ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n",
    "        mask : :class:`~torch.ByteTensor`, optional\n",
    "            Mask tensor of size ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,\n",
    "            ``(batch_size, seq_length)`` otherwise.\n",
    "        reduction : str, optional\n",
    "            Specifies  the reduction to apply to the output: 'none'|'sum'|'mean'|'token_mean'.\n",
    "            'none': no reduction will be applied. 'sum': the output will be summed over batches.\n",
    "            'mean': the output will be averaged over batches. 'token_mean': the output will be\n",
    "            averaged over tokens.\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`~torch.Tensor`\n",
    "            The log likelihood. This will have size ``(batch_size,)`` if reduction is 'none',\n",
    "            ``()`` otherwise.\n",
    "        \"\"\"\n",
    "        self._validate(emissions, tags=tags, mask=mask)\n",
    "        if reduction not in ('none', 'sum', 'mean', 'token_mean'):\n",
    "            raise ValueError(f'invalid reduction: {reduction}')\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(tags, dtype=torch.uint8)\n",
    "\n",
    "        if self.batch_first:\n",
    "            emissions = emissions.transpose(0, 1)\n",
    "            tags = tags.transpose(0, 1)\n",
    "            mask = mask.transpose(0, 1)\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        numerator = self._compute_score(emissions, tags, mask)\n",
    "        # shape: (batch_size,)\n",
    "        denominator = self._compute_normalizer(emissions, mask)\n",
    "        # shape: (batch_size,)\n",
    "        llh = numerator - denominator\n",
    "\n",
    "        if reduction == 'none':\n",
    "            return llh\n",
    "        if reduction == 'sum':\n",
    "            return llh.sum()\n",
    "        if reduction == 'mean':\n",
    "            return llh.mean()\n",
    "        assert reduction == 'token_mean'\n",
    "        return llh.sum() / mask.float().sum()\n",
    "\n",
    "    def decode(self, emissions: torch.Tensor,\n",
    "               mask: Optional[torch.ByteTensor] = None) -> List[List[int]]:\n",
    "        \"\"\"Find the most likely tag sequence using Viterbi algorithm.\n",
    "        Arguments\n",
    "        ---------\n",
    "        emissions : :class:`~torch.Tensor`\n",
    "            Emission score tensor of size ``(seq_length, batch_size, num_tags)`` if\n",
    "            ``batch_first`` is ``False``, ``(batch_size, seq_length, num_tags)`` otherwise.\n",
    "        mask : :class:`~torch.ByteTensor`, optional\n",
    "            Mask tensor of size ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,\n",
    "            ``(batch_size, seq_length)`` otherwise.\n",
    "        Returns\n",
    "        -------\n",
    "        List[List[int]]\n",
    "            List of list containing the best tag sequence for each batch.\n",
    "        \"\"\"\n",
    "        self._validate(emissions, mask=mask)\n",
    "        if mask is None:\n",
    "            mask = emissions.new_ones(emissions.shape[:2], dtype=torch.uint8)\n",
    "\n",
    "        if self.batch_first:\n",
    "            emissions = emissions.transpose(0, 1)\n",
    "            mask = mask.transpose(0, 1)\n",
    "\n",
    "        return self._viterbi_decode(emissions, mask)\n",
    "\n",
    "    def _validate(\n",
    "            self,\n",
    "            emissions: torch.Tensor,\n",
    "            tags: Optional[torch.LongTensor] = None,\n",
    "            mask: Optional[torch.ByteTensor] = None) -> None:\n",
    "        if emissions.dim() != 3:\n",
    "            raise ValueError(f'emissions must have dimension of 3, got {emissions.dim()}')\n",
    "        if emissions.size(2) != self.num_tags:\n",
    "            raise ValueError(\n",
    "                f'expected last dimension of emissions is {self.num_tags}, '\n",
    "                f'got {emissions.size(2)}')\n",
    "\n",
    "        if tags is not None:\n",
    "            if emissions.shape[:2] != tags.shape:\n",
    "                raise ValueError(\n",
    "                    'the first two dimensions of emissions and tags must match, '\n",
    "                    f'got {tuple(emissions.shape[:2])} and {tuple(tags.shape)}')\n",
    "\n",
    "        if mask is not None:\n",
    "            if emissions.shape[:2] != mask.shape:\n",
    "                raise ValueError(\n",
    "                    'the first two dimensions of emissions and mask must match, '\n",
    "                    f'got {tuple(emissions.shape[:2])} and {tuple(mask.shape)}')\n",
    "            no_empty_seq = not self.batch_first and mask[0].all()\n",
    "            no_empty_seq_bf = self.batch_first and mask[:, 0].all()\n",
    "            if not no_empty_seq and not no_empty_seq_bf:\n",
    "                raise ValueError('mask of the first timestep must all be on')\n",
    "\n",
    "    def _compute_score(\n",
    "            self, emissions: torch.Tensor, tags: torch.LongTensor,\n",
    "            mask: torch.ByteTensor) -> torch.Tensor:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # tags: (seq_length, batch_size)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and tags.dim() == 2\n",
    "        assert emissions.shape[:2] == tags.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask.shape == tags.shape\n",
    "        assert mask[0].all()\n",
    "\n",
    "        seq_length, batch_size = tags.shape\n",
    "        mask = mask.float()\n",
    "\n",
    "        # Start transition score and first emission\n",
    "        # shape: (batch_size,)\n",
    "        score = self.start_transitions[tags[0]]\n",
    "        score += emissions[0, torch.arange(batch_size), tags[0]]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Transition score to next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += self.transitions[tags[i - 1], tags[i]] * mask[i]\n",
    "\n",
    "            # Emission score for next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size,)\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "        # shape: (batch_size,)\n",
    "        last_tags = tags[seq_ends, torch.arange(batch_size)]\n",
    "        # shape: (batch_size,)\n",
    "        score += self.end_transitions[last_tags]\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _compute_normalizer(\n",
    "            self, emissions: torch.Tensor, mask: torch.ByteTensor) -> torch.Tensor:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and mask.dim() == 2\n",
    "        assert emissions.shape[:2] == mask.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask[0].all()\n",
    "\n",
    "        seq_length = emissions.size(0)\n",
    "\n",
    "        # Start transition score and first emission; score has size of\n",
    "        # (batch_size, num_tags) where for each batch, the j-th column stores\n",
    "        # the score that the first timestep has tag j\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emissions = emissions[i].unsqueeze(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the sum of scores of all\n",
    "            # possible tag sequences so far that end with transitioning from tag i to tag j\n",
    "            # and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emissions\n",
    "\n",
    "            # Sum over all possible current tags, but we're in score space, so a sum\n",
    "            # becomes a log-sum-exp: for each sample, entry i stores the sum of scores of\n",
    "            # all possible tag sequences so far, that end in tag i\n",
    "            # shape: (batch_size, num_tags)\n",
    "            next_score = torch.logsumexp(next_score, dim=1)\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Sum (log-sum-exp) over all possible tags\n",
    "        # shape: (batch_size,)\n",
    "        return torch.logsumexp(score, dim=1)\n",
    "\n",
    "    def _viterbi_decode(self, emissions: torch.FloatTensor,\n",
    "                        mask: torch.ByteTensor) -> List[List[int]]:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and mask.dim() == 2\n",
    "        assert emissions.shape[:2] == mask.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask[0].all()\n",
    "\n",
    "        seq_length, batch_size = mask.shape\n",
    "\n",
    "        # Start transition and first emission\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "        history = []\n",
    "\n",
    "        # score is a tensor of size (batch_size, num_tags) where for every batch,\n",
    "        # value at column j stores the score of the best tag sequence so far that ends\n",
    "        # with tag j\n",
    "        # history saves where the best tags candidate transitioned from; this is used\n",
    "        # when we trace back the best tag sequence\n",
    "\n",
    "        # Viterbi algorithm recursive case: we compute the score of the best tag sequence\n",
    "        # for every possible next tag\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast viterbi score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emission = emissions[i].unsqueeze(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the score of the best\n",
    "            # tag sequence so far that ends with transitioning from tag i to tag j and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emission\n",
    "\n",
    "            # Find the maximum score over all possible current tag\n",
    "            # shape: (batch_size, num_tags)\n",
    "            next_score, indices = next_score.max(dim=1)\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # and save the index that produces the next score\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "            history.append(indices)\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Now, compute the best path for each sample\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "        best_tags_list = []\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            # Find the tag which maximizes the score at the last timestep; this is our best tag\n",
    "            # for the last timestep\n",
    "            _, best_last_tag = score[idx].max(dim=0)\n",
    "            best_tags = [best_last_tag.item()]\n",
    "\n",
    "            # We trace back where the best last tag comes from, append that to our best tag\n",
    "            # sequence, and trace it back again, and so on\n",
    "            for hist in reversed(history[:seq_ends[idx]]):\n",
    "                best_last_tag = hist[idx][best_tags[-1]]\n",
    "                best_tags.append(best_last_tag.item())\n",
    "\n",
    "            # Reverse the order because we start from the last timestep\n",
    "            best_tags.reverse()\n",
    "            best_tags_list.append(best_tags)\n",
    "\n",
    "        return best_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Albert_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.albert = AutoModel.from_pretrained(\"voidful/albert_chinese_large\", return_dict=True)\n",
    "        \n",
    "        self.classifier = nn.Linear(self.albert.config.hidden_size, len(ner_tag2id))\n",
    "        \n",
    "        self.crf = CRF(len(ner_tag2id), batch_first=True, tag_id_to_name=ner_id2tag)\n",
    "        \n",
    "        self.CELoss_fn = nn.CrossEntropyLoss(ignore_index=ner_tag2id['[PAD]'])\n",
    "        \n",
    "    def forward(self, input_seqs, input_mask, targets):\n",
    "        \n",
    "        output = self.albert(input_ids=input_seqs, \n",
    "                             attention_mask=input_mask,)\n",
    "        \n",
    "        output = output['last_hidden_state']\n",
    "        \n",
    "        output = self.classifier(output)\n",
    "        \n",
    "        ce_loss = self.CELoss_fn(output.view(-1, len(ner_tag2id)), targets.view(-1))\n",
    "        \n",
    "        crf_loss = self.crf(emissions=output, tags=targets, mask=input_mask.byte(), reduction='mean')\n",
    "        \n",
    "        crf_loss = crf_loss*-1\n",
    "        \n",
    "        loss = ce_loss + 1e-2*crf_loss\n",
    "        \n",
    "        return output, ce_loss, crf_loss, loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Albert_CRF()\n",
    "\n",
    "print('total parms : ', sum(p.numel() for p in model.parameters()))\n",
    "print('trainable parms : ', sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribute data parallel\n",
    "\n",
    "# dist_backend = 'nccl'\n",
    "# dist_url = 'tcp://127.0.0.1:45655'\n",
    "# world_size = 1\n",
    "# rank = 0\n",
    "\n",
    "# torch.distributed.init_process_group(backend=dist_backend, \n",
    "#                                      init_method=dist_url, \n",
    "#                                      world_size=world_size, \n",
    "#                                      rank=rank)\n",
    "\n",
    "\n",
    "# bertlm = torch.nn.parallel.DistributedDataParallel(bertlm, find_unused_parameters=False)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "# dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = \"albert_ner/exp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = -1\n",
    "model_name = 'albert_ner_len{}_batch_{}'.format(opts.max_seq_len, opts.batch_size)\n",
    "now = str(datetime.now()).split('.')[0]\n",
    "experiment_name = '{}_{}'.format(model_name, now)\n",
    "experiment_dir = Path(exp_dir) / experiment_name\n",
    "experiment_dir.mkdir(exist_ok=True, parents=True)\n",
    "print(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2file(log_file, msg):\n",
    "    with open(log_file, 'a') as fw:\n",
    "        fw.write(msg)\n",
    "        fw.write('\\n')\n",
    "\n",
    "experiment_trainlog = experiment_dir / 'train_log.txt'\n",
    "experiment_devlog = experiment_dir / 'dev_log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opts.learning_rate)\n",
    "print(opts.bert_lr)\n",
    "\n",
    "optimizer = transformers.AdamW([\n",
    "    {'params': model.module.parameters(), 'lr':opts.learning_rate},\n",
    "], lr=opts.learning_rate)\n",
    "\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
    "                                                         num_warmup_steps=opts.warmup_steps, \n",
    "                                                         num_training_steps=len(train_iter)*opts.num_epochs)\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss(ignore_index=bert_tokenizer.pad_token_id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0.001, exp_dir=''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.best_epoch = 0\n",
    "        self.exp_dir=Path(exp_dir)\n",
    "\n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, epoch)\n",
    "        elif score < self.best_score:\n",
    "#         elif score < self.best_score or score < self.best_score * (1-self.delta):\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                msg = 'best epoch : {}'.format(self.best_epoch)\n",
    "                print(msg)\n",
    "                log2file(self.exp_dir / 'train_log.txt', msg)\n",
    "                (self.exp_dir / 'best_model').symlink_to(self.exp_dir / 'epoch_{}.mdl'.format(self.best_epoch))\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint(val_loss, model, epoch)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, epoch):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            msg = f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})'\n",
    "            print(msg)\n",
    "            log2file(self.exp_dir / 'train_log.txt', msg)\n",
    "#         torch.save(model.state_dict(), self.exp_dir / 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=50, verbose=True, exp_dir=str(experiment_dir))\n",
    "\n",
    "for k,v in opts.items():\n",
    "    log_msg = '- {}: {}'.format(k, v)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    print(log_msg)\n",
    "    \n",
    "pbar_train = tqdm.notebook.tqdm(total=len(train_iter))\n",
    "pbar_dev = tqdm.notebook.tqdm(total=len(dev_iter))\n",
    "    \n",
    "log_msg = '='*50\n",
    "print(log_msg)\n",
    "log2file(str(experiment_trainlog), log_msg)\n",
    "log_msg = 'optim : \\n' + str(optimizer)\n",
    "print(log_msg)   \n",
    "log2file(str(experiment_trainlog), log_msg)\n",
    "\n",
    "\n",
    "s = 5\n",
    "checkpoint = [int(len(train_iter)/s*i) for i in range(1, s)]\n",
    "\n",
    "oom_time = 0\n",
    "\n",
    "print('check point : ', checkpoint)\n",
    "\n",
    "for epoch in range(last_epoch+1,  opts.num_epochs, 1):\n",
    "    \n",
    "    pbar_train.reset()\n",
    "    pbar_dev.reset()\n",
    "    \n",
    "    log_msg = '='*50\n",
    "    print(log_msg)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    loss_tracker = []\n",
    "    celoss_tracker = []\n",
    "    crfloss_tracker = []\n",
    "    time_tracker = []\n",
    "    time_tracker.append(time.time())\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    \n",
    "    for iteration, batch in enumerate(train_iter):\n",
    "        \n",
    "        inputs, labels, inputs_ids, labels_ids, input_seqs, input_mask, targets, lens = batch\n",
    "        \n",
    "        batch_size = input_seqs.size(0)\n",
    "        assert(batch_size == targets.size(0))\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_seqs = input_seqs.cuda()\n",
    "            input_mask = input_mask.cuda()\n",
    "    #         lens = lens.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        try:\n",
    "\n",
    "            output, ce_loss, crf_loss, loss = model(input_seqs, input_mask, targets)\n",
    "            \n",
    "            ce_loss = ce_loss.mean()\n",
    "            crf_loss = crf_loss.mean()\n",
    "            loss = loss.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), opts.max_grad_norm)\n",
    "\n",
    "            if (iteration + 1) % opts.gradient_accumulation == 0 or iteration == len(train_iter)-1:\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                \n",
    "\n",
    "            loss_tracker.append(loss.item()*batch_size)\n",
    "            celoss_tracker.append(ce_loss.item()*batch_size)\n",
    "            crfloss_tracker.append(crf_loss.item()*batch_size)\n",
    "\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "        except RuntimeError as exception:\n",
    "            \n",
    "            if \"out of memory\" in str(exception):\n",
    "                oom_time += 1\n",
    "                log_msg = \"WARNING: ran out of memory,times: {}\".format(oom_time)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "                torch.cuda.empty_cache()\n",
    "                if hasattr(torch.cuda, 'empty_cache'):\n",
    "                    torch.cuda.empty_cache()\n",
    "            elif \"Gather got an\" in str(exception):\n",
    "                log_msg = str(exception)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "            else:\n",
    "                log_msg = str(exception)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "                raise exception\n",
    "                \n",
    "        #=================================\n",
    "        \n",
    "        if global_step in checkpoint:\n",
    "            \n",
    "            now_time = time.time()\n",
    "            time_tracker.append(time.time())\n",
    "            cur_avg_loss = np.sum(np.array(loss_tracker)) / (global_step * opts.batch_size)\n",
    "            cur_avg_celoss = np.sum(np.array(celoss_tracker)) / (global_step * opts.batch_size)\n",
    "            cur_avg_crfloss = np.sum(np.array(crfloss_tracker)) / (global_step * opts.batch_size)\n",
    "            log_msg = \"{} | Batch {:d}/{:d} | Mean Loss {:5.5f} | Mean CE Loss {:5.5f} | Mean CRF Loss {:5.5f} | time cost {:d} s\"  \\\n",
    "                    .format('train'.upper(), global_step, len(train_iter), cur_avg_loss, cur_avg_celoss, cur_avg_crfloss, int(time_tracker[-1] - time_tracker[-2]))\n",
    "            print(log_msg)\n",
    "            log2file(str(experiment_trainlog), log_msg)\n",
    "            now_percent = checkpoint.index(global_step)+1\n",
    "            \n",
    "            ckpt = {\n",
    "                \"net\": model.state_dict(),\n",
    "                'optimizer':optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }            \n",
    "            \n",
    "            torch.save(ckpt, experiment_dir / 'epoch_{}_{}.ckpt'.format(epoch-1, now_percent))\n",
    "            \n",
    "            dev_loss_tracker = []\n",
    "            dev_celoss_tracker = []\n",
    "            dev_crfloss_tracker = []\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            pbar_dev.reset()\n",
    "\n",
    "            for iteration, batch in enumerate(dev_iter):\n",
    "\n",
    "                inputs, labels, inputs_ids, labels_ids, input_seqs, input_mask, targets, lens = batch\n",
    "\n",
    "                batch_size = input_seqs.size(0)\n",
    "                assert(batch_size == targets.size(0))\n",
    "\n",
    "                if USE_CUDA:\n",
    "                    input_seqs = input_seqs.cuda()\n",
    "                    input_mask = input_mask.cuda()\n",
    "            #         lens = lens.cuda()\n",
    "                    targets = targets.cuda()\n",
    "\n",
    "                model.eval()\n",
    "\n",
    "                try:\n",
    "\n",
    "                    output, ce_loss, crf_loss, loss = model(input_seqs, input_mask, targets)\n",
    "                    \n",
    "                    ce_loss = ce_loss.mean()\n",
    "                    crf_loss = crf_loss.mean()\n",
    "                    loss = loss.mean()\n",
    "\n",
    "                    dev_loss_tracker.append(loss.item()*batch_size)\n",
    "                    dev_celoss_tracker.append(ce_loss.item()*batch_size)\n",
    "                    dev_crfloss_tracker.append(crf_loss.item()*batch_size)\n",
    "\n",
    "                except RuntimeError as exception:\n",
    "\n",
    "                    if \"out of memory\" in str(exception):\n",
    "                        oom_time += 1\n",
    "                        log_msg = \"WARNING: ran out of memory,times: {}\".format(oom_time)\n",
    "                        print(log_msg)   \n",
    "                        log2file(str(experiment_trainlog), log_msg)\n",
    "                        torch.cuda.empty_cache()\n",
    "                        if hasattr(torch.cuda, 'empty_cache'):\n",
    "                            torch.cuda.empty_cache()\n",
    "                    elif \"Gather got an\" in str(exception):\n",
    "                        log_msg = str(exception)\n",
    "                        print(log_msg)   \n",
    "                        log2file(str(experiment_trainlog), log_msg)\n",
    "                    else:\n",
    "                        log_msg = str(exception)\n",
    "                        print(log_msg)   \n",
    "                        log2file(str(experiment_trainlog), log_msg)\n",
    "                        raise exception\n",
    "#                 torch.cuda.empty_cache()\n",
    "                        \n",
    "                pbar_dev.update(1)\n",
    "\n",
    "\n",
    "            total_time = time.time() - start\n",
    "\n",
    "            mean_loss = np.sum(np.array(dev_loss_tracker)) / dev_dataset.__len__()\n",
    "            mean_celoss = np.sum(np.array(dev_celoss_tracker)) / dev_dataset.__len__()\n",
    "            mean_crfloss = np.sum(np.array(dev_crfloss_tracker)) / dev_dataset.__len__()\n",
    "            log_msg = \"{}   | Batch {:d}/{:d} | Mean Loss {:5.5f} | Mean CE Loss {:5.5f} | Mean CRF Loss {:5.5f} | Total time cost {:d} s\"  \\\n",
    "                .format('dev'.upper(), global_step, len(train_iter), mean_loss, mean_celoss, mean_crfloss, int(total_time))\n",
    "            print(log_msg)\n",
    "            log2file(str(experiment_trainlog), log_msg)\n",
    "\n",
    "            val_loss = mean_loss\n",
    "\n",
    "            early_stopping(val_loss, model, epoch)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "        \n",
    "        global_step += 1\n",
    "        pbar_train.update(1)\n",
    "\n",
    "    \n",
    "    total_time = time.time() - time_tracker[0]    \n",
    "    mean_loss = np.sum(np.array(loss_tracker)) / train_dataset.__len__()\n",
    "    mean_celoss = np.sum(np.array(dev_celoss_tracker)) / dev_dataset.__len__()\n",
    "    mean_crfloss = np.sum(np.array(dev_crfloss_tracker)) / dev_dataset.__len__()\n",
    "    log_msg = \"{} | Epoch {:d}/{:d} | Mean Loss {:5.5f} | Mean CE Loss {:5.5f} | Mean CRF Loss {:5.5f} | Total time cost {:d} s\"  \\\n",
    "        .format('train'.upper(), epoch, opts.num_epochs, mean_loss, mean_celoss, mean_crfloss, int(total_time))\n",
    "    print(log_msg)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "\n",
    "    #-----------------------\n",
    "\n",
    "    loss_tracker = []\n",
    "    celoss_tracker = []\n",
    "    crfloss_tracker = []\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    pbar_dev.reset()\n",
    "\n",
    "    for iteration, batch in enumerate(dev_iter):\n",
    "\n",
    "        inputs, labels, inputs_ids, labels_ids, input_seqs, input_mask, targets, lens = batch\n",
    "        \n",
    "        batch_size = input_seqs.size(0)\n",
    "        assert(batch_size == targets.size(0))\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_seqs = input_seqs.cuda()\n",
    "            input_mask = input_mask.cuda()\n",
    "    #         lens = lens.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            output, ce_loss, crf_loss, loss = model(input_seqs, input_mask, targets)\n",
    "            \n",
    "            ce_loss = ce_loss.mean()\n",
    "            crf_loss = crf_loss.mean()\n",
    "            loss = loss.mean()\n",
    "\n",
    "            loss_tracker.append(loss.item()*batch_size)\n",
    "            celoss_tracker.append(ce_loss.item()*batch_size)\n",
    "            crfloss_tracker.append(crf_loss.item()*batch_size)\n",
    "            \n",
    "        except RuntimeError as exception:\n",
    "            \n",
    "            if \"out of memory\" in str(exception):\n",
    "                oom_time += 1\n",
    "                log_msg = \"WARNING: ran out of memory,times: {}\".format(oom_time)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "                torch.cuda.empty_cache()\n",
    "                if hasattr(torch.cuda, 'empty_cache'):\n",
    "                    torch.cuda.empty_cache()\n",
    "            elif \"Gather got an\" in str(exception):\n",
    "                log_msg = str(exception)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "            else:\n",
    "                log_msg = str(exception)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "                raise exception\n",
    "                \n",
    "#         torch.cuda.empty_cache()\n",
    "        pbar_dev.update(1)\n",
    "\n",
    "    total_time = time.time() - start\n",
    "\n",
    "    mean_loss = np.sum(np.array(loss_tracker)) / dev_dataset.__len__()\n",
    "    mean_celoss = np.sum(np.array(celoss_tracker)) / dev_dataset.__len__()\n",
    "    mean_crfloss = np.sum(np.array(crfloss_tracker)) / dev_dataset.__len__()\n",
    "    log_msg = \"{}   | Epoch {:d}/{:d} | Mean Loss {:5.5f}  | Mean CE Loss {:5.5f}  | Mean CRF Loss {:5.5f}  | Total time cost {:d} s\"  \\\n",
    "        .format('dev'.upper(), epoch, opts.num_epochs, mean_loss, mean_celoss, mean_crfloss, int(total_time))\n",
    "    print(log_msg)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    \n",
    "    val_loss = mean_loss\n",
    "    \n",
    "    early_stopping(val_loss, model, epoch)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "    ckpt = {\n",
    "        \"net\": model.state_dict(),\n",
    "        'optimizer':optimizer.state_dict(),\n",
    "        \"epoch\": epoch\n",
    "    } \n",
    "        \n",
    "    torch.save(ckpt, experiment_dir / 'epoch_{}.ckpt'.format(epoch))\n",
    "\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
