{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, BertTokenizer\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_a = BertTokenizer.from_pretrained(\"voidful/albert_chinese_large\")\n",
    "tokenizer_b = BertTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokenizer_a.vocab) == len(tokenizer_b.vocab)\n",
    "\n",
    "for a, b in zip(tokenizer_a.vocab, tokenizer_b.vocab):\n",
    "    \n",
    "    if a != b:\n",
    "        print(a, b)\n",
    "#     print(a, b)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./config/vocab.txt',\n",
       " './config/special_tokens_map.json',\n",
       " './config/added_tokens.json')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_a.save_pretrained('./config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# albert_tokenizer = AutoTokenizer.from_pretrained('voidful/albert_chinese_large')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"voidful/albert_chinese_large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    }
   ],
   "source": [
    "ner_tag_file = \"pos.tgt.dict\"\n",
    "ner_tag2id = {}\n",
    "\n",
    "count = 0\n",
    "with open(ner_tag_file) as fp:\n",
    "    for line in fp:\n",
    "        line = line.strip().split()\n",
    "        ner_tag2id[line[0]] = count\n",
    "        count += 1\n",
    "        \n",
    "ner_id2tag = {v:k for k, v in ner_tag2id.items()}\n",
    "        \n",
    "print(len(ner_tag2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Mapping\n",
    "\n",
    "class CRF(nn.Module):\n",
    "    \"\"\"Conditional random field.\n",
    "    This module implements a conditional random field [LMP]. The forward computation\n",
    "    of this class computes the log likelihood of the given sequence of tags and\n",
    "    emission score tensor. This class also has ``decode`` method which finds the\n",
    "    best tag sequence given an emission score tensor using `Viterbi algorithm`_.\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_tags : int\n",
    "        Number of tags.\n",
    "    batch_first : bool, optional\n",
    "        Whether the first dimension corresponds to the size of a minibatch.\n",
    "    Attributes\n",
    "    ----------\n",
    "    start_transitions : :class:`~torch.nn.Parameter`\n",
    "        Start transition score tensor of size ``(num_tags,)``.\n",
    "    end_transitions : :class:`~torch.nn.Parameter`\n",
    "        End transition score tensor of size ``(num_tags,)``.\n",
    "    transitions : :class:`~torch.nn.Parameter`\n",
    "        Transition score tensor of size ``(num_tags, num_tags)``.\n",
    "    References\n",
    "    ----------\n",
    "    .. [LMP] Lafferty, J., McCallum, A., Pereira, F. (2001).\n",
    "             \"Conditional random fields: Probabilistic models for segmenting and\n",
    "             labeling sequence data\". *Proc. 18th International Conf. on Machine\n",
    "             Learning*. Morgan Kaufmann. pp. 282–289.\n",
    "    .. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_tags: int, batch_first: bool = False, tag_id_to_name: Mapping[int, str] = None) -> None:\n",
    "        if num_tags <= 0:\n",
    "            raise ValueError(f'invalid number of tags: {num_tags}')\n",
    "        super().__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.batch_first = batch_first\n",
    "        self.start_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "        self.end_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "        self.transitions = nn.Parameter(torch.empty(num_tags, num_tags))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        if not tag_id_to_name: return\n",
    "\n",
    "        self.tag_id_to_name = dict()\n",
    "        for t_i_n in tag_id_to_name.items():\n",
    "            t_i, t_name = t_i_n\n",
    "            t_name_part = t_name.split('-')\n",
    "            assert len(t_name_part) <= 3, \"tag name {} error\".format(t_name)\n",
    "            t_pref = t_name_part[0]\n",
    "            t_suff = t_name_part[1] if len(t_name_part) > 1 else t_name_part[0]\n",
    "            if len(t_name_part) == 3:\n",
    "                t_suff += '-'\n",
    "                t_suff += t_name_part[2]\n",
    "            self.tag_id_to_name[t_i] = (t_pref, t_suff)\n",
    "\n",
    "        self.set_transition_constraint()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Initialize the transition parameters.\n",
    "        The parameters will be initialized randomly from a uniform distribution\n",
    "        between -0.01 and 0.01.\n",
    "        \"\"\"\n",
    "        nn.init.uniform_(self.start_transitions, -0.01, 0.01)\n",
    "        nn.init.uniform_(self.end_transitions, -0.01, 0.01)\n",
    "        nn.init.uniform_(self.transitions, -0.01, 0.01)\n",
    "\n",
    "    def set_transition_constraint(self) -> None:\n",
    "        \"\"\"Set impossible transitions\n",
    "        Set transitions between impossible labels to a very low score, effectively disabling them\n",
    "        \"\"\"\n",
    "        if len(self.tag_id_to_name) < 1:\n",
    "            print(\"no tag id to name dict\")\n",
    "            return\n",
    "        for source_i, (source_pref, source_suff) in self.tag_id_to_name.items():\n",
    "            if source_pref in ['[PAD]', '[SEP]']:\n",
    "                self.start_transitions.data[source_i] = -10000.\n",
    "                # can only go to PAD\n",
    "                self.transitions.data[source_i].fill_(-10000.)\n",
    "                for targ_i in range(len(self.tag_id_to_name)):\n",
    "                    targ_pref, targ_suff = self.tag_id_to_name[targ_i]\n",
    "                    if (targ_pref in ['[PAD]']):\n",
    "                        self.transitions.data[source_i, targ_i] = 0.001\n",
    "            if source_pref in ['[CLS]']:\n",
    "                # possible ends are S- and B-\n",
    "                self.transitions.data[source_i].fill_(-10000.)\n",
    "                for targ_i in range(len(self.tag_id_to_name)):\n",
    "                    targ_pref, targ_suff = self.tag_id_to_name[targ_i]\n",
    "                    if targ_pref in ['S', 'B']:\n",
    "                        self.transitions.data[source_i, targ_i] = 0.001\n",
    "                    if targ_pref in ['I', 'E']:\n",
    "                        self.transitions.data[source_i, targ_i] = -10000.\n",
    "            if source_pref in ['B', 'I']:\n",
    "                # possible ends are I- and E-\n",
    "                self.transitions.data[source_i].fill_(-10000.)\n",
    "                for targ_i in range(len(self.tag_id_to_name)):\n",
    "                    targ_pref, targ_suff = self.tag_id_to_name[targ_i]\n",
    "                    if (targ_suff == source_suff) and (targ_pref in ['I', 'E']):\n",
    "                        self.transitions.data[source_i, targ_i] = 0.001\n",
    "            if source_pref in ['S', 'E']:\n",
    "                # cannot go to I or E\n",
    "                for targ_i in range(len(self.tag_id_to_name)):\n",
    "                    targ_pref, targ_suff = self.tag_id_to_name[targ_i]\n",
    "                    if targ_pref in ['I', 'E']:\n",
    "                        self.transitions.data[source_i, targ_i] = -10000.\n",
    "            if source_pref in ['I', 'E']:\n",
    "                # cannot be start transitions\n",
    "                self.start_transitions.data[source_i] = -10000.\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(num_tags={self.num_tags})'\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            emissions: torch.Tensor,\n",
    "            tags: torch.LongTensor,\n",
    "            mask: Optional[torch.ByteTensor] = None,\n",
    "            reduction: str = 'sum',\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the conditional log likelihood of a sequence of tags given emission scores.\n",
    "        Arguments\n",
    "        ---------\n",
    "        emissions : :class:`~torch.Tensor`\n",
    "            Emission score tensor of size ``(seq_length, batch_size, num_tags)`` if\n",
    "            ``batch_first`` is ``False``, ``(batch_size, seq_length, num_tags)`` otherwise.\n",
    "        tags : :class:`~torch.LongTensor`\n",
    "            Sequence of tags tensor of size ``(seq_length, batch_size)`` if\n",
    "            ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n",
    "        mask : :class:`~torch.ByteTensor`, optional\n",
    "            Mask tensor of size ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,\n",
    "            ``(batch_size, seq_length)`` otherwise.\n",
    "        reduction : str, optional\n",
    "            Specifies  the reduction to apply to the output: 'none'|'sum'|'mean'|'token_mean'.\n",
    "            'none': no reduction will be applied. 'sum': the output will be summed over batches.\n",
    "            'mean': the output will be averaged over batches. 'token_mean': the output will be\n",
    "            averaged over tokens.\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`~torch.Tensor`\n",
    "            The log likelihood. This will have size ``(batch_size,)`` if reduction is 'none',\n",
    "            ``()`` otherwise.\n",
    "        \"\"\"\n",
    "        self._validate(emissions, tags=tags, mask=mask)\n",
    "        if reduction not in ('none', 'sum', 'mean', 'token_mean'):\n",
    "            raise ValueError(f'invalid reduction: {reduction}')\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(tags, dtype=torch.uint8)\n",
    "\n",
    "        if self.batch_first:\n",
    "            emissions = emissions.transpose(0, 1)\n",
    "            tags = tags.transpose(0, 1)\n",
    "            mask = mask.transpose(0, 1)\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        numerator = self._compute_score(emissions, tags, mask)\n",
    "        # shape: (batch_size,)\n",
    "        denominator = self._compute_normalizer(emissions, mask)\n",
    "        # shape: (batch_size,)\n",
    "        llh = numerator - denominator\n",
    "\n",
    "        if reduction == 'none':\n",
    "            return llh\n",
    "        if reduction == 'sum':\n",
    "            return llh.sum()\n",
    "        if reduction == 'mean':\n",
    "            return llh.mean()\n",
    "        assert reduction == 'token_mean'\n",
    "        return llh.sum() / mask.float().sum()\n",
    "\n",
    "    def decode(self, emissions: torch.Tensor,\n",
    "               mask: Optional[torch.ByteTensor] = None) -> List[List[int]]:\n",
    "        \"\"\"Find the most likely tag sequence using Viterbi algorithm.\n",
    "        Arguments\n",
    "        ---------\n",
    "        emissions : :class:`~torch.Tensor`\n",
    "            Emission score tensor of size ``(seq_length, batch_size, num_tags)`` if\n",
    "            ``batch_first`` is ``False``, ``(batch_size, seq_length, num_tags)`` otherwise.\n",
    "        mask : :class:`~torch.ByteTensor`, optional\n",
    "            Mask tensor of size ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,\n",
    "            ``(batch_size, seq_length)`` otherwise.\n",
    "        Returns\n",
    "        -------\n",
    "        List[List[int]]\n",
    "            List of list containing the best tag sequence for each batch.\n",
    "        \"\"\"\n",
    "        self._validate(emissions, mask=mask)\n",
    "        if mask is None:\n",
    "            mask = emissions.new_ones(emissions.shape[:2], dtype=torch.uint8)\n",
    "\n",
    "        if self.batch_first:\n",
    "            emissions = emissions.transpose(0, 1)\n",
    "            mask = mask.transpose(0, 1)\n",
    "\n",
    "        return self._viterbi_decode(emissions, mask)\n",
    "\n",
    "    def _validate(\n",
    "            self,\n",
    "            emissions: torch.Tensor,\n",
    "            tags: Optional[torch.LongTensor] = None,\n",
    "            mask: Optional[torch.ByteTensor] = None) -> None:\n",
    "        if emissions.dim() != 3:\n",
    "            raise ValueError(f'emissions must have dimension of 3, got {emissions.dim()}')\n",
    "        if emissions.size(2) != self.num_tags:\n",
    "            raise ValueError(\n",
    "                f'expected last dimension of emissions is {self.num_tags}, '\n",
    "                f'got {emissions.size(2)}')\n",
    "\n",
    "        if tags is not None:\n",
    "            if emissions.shape[:2] != tags.shape:\n",
    "                raise ValueError(\n",
    "                    'the first two dimensions of emissions and tags must match, '\n",
    "                    f'got {tuple(emissions.shape[:2])} and {tuple(tags.shape)}')\n",
    "\n",
    "        if mask is not None:\n",
    "            if emissions.shape[:2] != mask.shape:\n",
    "                raise ValueError(\n",
    "                    'the first two dimensions of emissions and mask must match, '\n",
    "                    f'got {tuple(emissions.shape[:2])} and {tuple(mask.shape)}')\n",
    "            no_empty_seq = not self.batch_first and mask[0].all()\n",
    "            no_empty_seq_bf = self.batch_first and mask[:, 0].all()\n",
    "            if not no_empty_seq and not no_empty_seq_bf:\n",
    "                raise ValueError('mask of the first timestep must all be on')\n",
    "\n",
    "    def _compute_score(\n",
    "            self, emissions: torch.Tensor, tags: torch.LongTensor,\n",
    "            mask: torch.ByteTensor) -> torch.Tensor:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # tags: (seq_length, batch_size)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and tags.dim() == 2\n",
    "        assert emissions.shape[:2] == tags.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask.shape == tags.shape\n",
    "        assert mask[0].all()\n",
    "\n",
    "        seq_length, batch_size = tags.shape\n",
    "        mask = mask.float()\n",
    "\n",
    "        # Start transition score and first emission\n",
    "        # shape: (batch_size,)\n",
    "        score = self.start_transitions[tags[0]]\n",
    "        score += emissions[0, torch.arange(batch_size), tags[0]]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Transition score to next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += self.transitions[tags[i - 1], tags[i]] * mask[i]\n",
    "\n",
    "            # Emission score for next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size,)\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "        # shape: (batch_size,)\n",
    "        last_tags = tags[seq_ends, torch.arange(batch_size)]\n",
    "        # shape: (batch_size,)\n",
    "        score += self.end_transitions[last_tags]\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _compute_normalizer(\n",
    "            self, emissions: torch.Tensor, mask: torch.ByteTensor) -> torch.Tensor:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and mask.dim() == 2\n",
    "        assert emissions.shape[:2] == mask.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask[0].all()\n",
    "\n",
    "        seq_length = emissions.size(0)\n",
    "\n",
    "        # Start transition score and first emission; score has size of\n",
    "        # (batch_size, num_tags) where for each batch, the j-th column stores\n",
    "        # the score that the first timestep has tag j\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emissions = emissions[i].unsqueeze(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the sum of scores of all\n",
    "            # possible tag sequences so far that end with transitioning from tag i to tag j\n",
    "            # and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emissions\n",
    "\n",
    "            # Sum over all possible current tags, but we're in score space, so a sum\n",
    "            # becomes a log-sum-exp: for each sample, entry i stores the sum of scores of\n",
    "            # all possible tag sequences so far, that end in tag i\n",
    "            # shape: (batch_size, num_tags)\n",
    "            next_score = torch.logsumexp(next_score, dim=1)\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Sum (log-sum-exp) over all possible tags\n",
    "        # shape: (batch_size,)\n",
    "        return torch.logsumexp(score, dim=1)\n",
    "\n",
    "    def _viterbi_decode(self, emissions: torch.FloatTensor,\n",
    "                        mask: torch.ByteTensor) -> List[List[int]]:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and mask.dim() == 2\n",
    "        assert emissions.shape[:2] == mask.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask[0].all()\n",
    "\n",
    "        seq_length, batch_size = mask.shape\n",
    "\n",
    "        # Start transition and first emission\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "        history = []\n",
    "\n",
    "        # score is a tensor of size (batch_size, num_tags) where for every batch,\n",
    "        # value at column j stores the score of the best tag sequence so far that ends\n",
    "        # with tag j\n",
    "        # history saves where the best tags candidate transitioned from; this is used\n",
    "        # when we trace back the best tag sequence\n",
    "\n",
    "        # Viterbi algorithm recursive case: we compute the score of the best tag sequence\n",
    "        # for every possible next tag\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast viterbi score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emission = emissions[i].unsqueeze(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the score of the best\n",
    "            # tag sequence so far that ends with transitioning from tag i to tag j and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emission\n",
    "\n",
    "            # Find the maximum score over all possible current tag\n",
    "            # shape: (batch_size, num_tags)\n",
    "            next_score, indices = next_score.max(dim=1)\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # and save the index that produces the next score\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "            history.append(indices)\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Now, compute the best path for each sample\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "        best_tags_list = []\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            # Find the tag which maximizes the score at the last timestep; this is our best tag\n",
    "            # for the last timestep\n",
    "            _, best_last_tag = score[idx].max(dim=0)\n",
    "            best_tags = [best_last_tag.item()]\n",
    "\n",
    "            # We trace back where the best last tag comes from, append that to our best tag\n",
    "            # sequence, and trace it back again, and so on\n",
    "            for hist in reversed(history[:seq_ends[idx]]):\n",
    "                best_last_tag = hist[idx][best_tags[-1]]\n",
    "                best_tags.append(best_last_tag.item())\n",
    "\n",
    "            # Reverse the order because we start from the last timestep\n",
    "            best_tags.reverse()\n",
    "            best_tags_list.append(best_tags)\n",
    "\n",
    "        return best_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Albert_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.albert = AutoModel.from_pretrained(\"voidful/albert_chinese_large\", return_dict=True)\n",
    "        \n",
    "        self.classifier = nn.Linear(self.albert.config.hidden_size, len(ner_tag2id))\n",
    "        \n",
    "        self.crf = CRF(len(ner_tag2id), batch_first=True, tag_id_to_name=ner_id2tag)\n",
    "        \n",
    "    def forward(self, input_seqs, input_mask):\n",
    "        \n",
    "        output = self.albert(input_ids=input_seqs, \n",
    "                             attention_mask=input_mask,)\n",
    "        \n",
    "        output = output['last_hidden_state']\n",
    "        \n",
    "        output = self.classifier(output)\n",
    "        \n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        \n",
    "        output = output.transpose(0, 1)\n",
    "        input_mask = input_mask.transpose(0, 1)\n",
    "        \n",
    "        best_tags_list = self.crf._viterbi_decode(emissions=output, mask=input_mask.byte())\n",
    "        \n",
    "        return best_tags_list\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parms :  16825630\n",
      "trainable parms :  0\n"
     ]
    }
   ],
   "source": [
    "exp_dir = Path(\"albert_ner/exp/\")\n",
    "\n",
    "model_path = exp_dir / 'albert_ner_len256_batch_20_2020-10-05 10:41:11/epoch_2_1.ckpt'\n",
    "\n",
    "model_path = \"albert_large.ckpt\"\n",
    "\n",
    "from collections import OrderedDict\n",
    "ckpt = torch.load(model_path, map_location='cpu')\n",
    "state_dict = ckpt['net']\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "    \n",
    "\n",
    "model = Albert_CRF()\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "for parms in model.parameters():\n",
    "    parms.requires_grad = False\n",
    "\n",
    "print('total parms : ', sum(p.numel() for p in model.parameters()))\n",
    "print('trainable parms : ', sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Albert_CRF(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=1024, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (ffn_output): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=222, bias=True)\n",
       "  (crf): CRF(num_tags=222)\n",
       ")"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \\\n",
    "'高加索地區的亞塞拜然與亞美尼亞兩國，9月27日為了主權爭議的納哥諾卡拉巴克地區（簡稱納卡）爆發衝突，雙方坦克、大砲與無人機齊發，連番交火，已知有250人喪命。10月5日再度互控對方攻擊平民區，亞塞拜然境內第二大城甘賈（Ganja）、亞美尼亞控制的納卡地區首府史提帕納科特（Stepanakert）分別遭砲火攻擊。'\n",
    "\n",
    "sent = '[CLS] ' + sent + ' [SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "token = tokenizer.tokenize(sent)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "print(len(token))\n",
    "\n",
    "input_seq = torch.LongTensor(ids)\n",
    "input_mask = torch.ones(input_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tags_list = model(input_seq.unsqueeze(0), input_mask.unsqueeze(0))\n",
    "best_tags_list = best_tags_list[0]\n",
    "best_tags_list = [ner_id2tag[tag_id] for tag_id in best_tags_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(best_tags_list) != len(token):\n",
    "    print('something error')\n",
    "\n",
    "toks = []\n",
    "tags = []\n",
    "\n",
    "temp_tok = ''\n",
    "  \n",
    "for tok, tag in zip(token[1:-1], best_tags_list[1:-1]):\n",
    "    \n",
    "    tag = tag.split('-')\n",
    "    \n",
    "    if len(tag) == 2:\n",
    "        bound, pos = tag\n",
    "    elif len(tag) == 3:\n",
    "        bound, _, pos = tag\n",
    "        \n",
    "    if bound == 'S':\n",
    "        toks.append(tok)\n",
    "        tags.append(pos)\n",
    "        \n",
    "        continue\n",
    "        \n",
    "    temp_tok += tok+' '\n",
    "    \n",
    "    if bound == 'B':\n",
    "        tags.append(pos)\n",
    "        \n",
    "    if bound == 'E':\n",
    "        temp_tok = temp_tok.replace(' ##', '')\n",
    "        if temp_tok.replace(' ', '').encode().isalpha() == False:\n",
    "            temp_tok = temp_tok.replace(' ', '')\n",
    "        toks.append(temp_tok)\n",
    "        temp_tok = ''\n",
    "\n",
    "        \n",
    "if len(temp_tok) > 0: # special case: no 'E-' in predicted_pos\n",
    "    toks.append(temp_tok)\n",
    "    temp_tok = ''\n",
    "    \n",
    "if len(toks) != len(tags):\n",
    "    print('something error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高加索 LOC\n",
      "地區 Nc\n",
      "的 DE\n",
      "亞塞拜然 LOC\n",
      "與 Caa\n",
      "亞美尼亞 LOC\n",
      "兩 Neu\n",
      "國 Nc\n",
      "， COMMACATEGORY\n",
      "9月 Nd\n",
      "27日 Nd\n",
      "為了 P\n",
      "主權 Na\n",
      "爭議 Na\n",
      "的 DE\n",
      "納哥諾卡拉巴克 LOC\n",
      "地區 Nc\n",
      "（ PARENTHESISCATEGORY\n",
      "簡稱 VG\n",
      "納卡 Nb\n",
      "） PARENTHESISCATEGORY\n",
      "爆發 VJ\n",
      "衝突 Na\n",
      "， COMMACATEGORY\n",
      "雙方 Nh\n",
      "坦克 Na\n",
      "、 PAUSECATEGORY\n",
      "大砲 Na\n",
      "與 Caa\n",
      "無人機 Na\n",
      "齊發 VH\n",
      "， COMMACATEGORY\n",
      "連番 D\n",
      "交火 VA\n",
      "， COMMACATEGORY\n",
      "已 D\n",
      "知 VK\n",
      "有 V_2\n",
      "250 Neu\n",
      "人 Na\n",
      "喪命 VH\n",
      "。 PERIODCATEGORY\n",
      "10月 Nd\n",
      "5日 Nd\n",
      "再度 D\n",
      "互控 VC\n",
      "對方 Nh\n",
      "攻擊 VC\n",
      "平民區 Nc\n",
      "， COMMACATEGORY\n",
      "亞塞拜然 LOC\n",
      "境 Na\n",
      "內 Ncd\n",
      "第二 Neu\n",
      "大城 Na\n",
      "甘賈 PER\n",
      "（ PARENTHESISCATEGORY\n",
      "ganja  PER\n",
      "） PARENTHESISCATEGORY\n",
      "、 PAUSECATEGORY\n",
      "亞美尼亞 LOC\n",
      "控制 VC\n",
      "的 DE\n",
      "納卡 LOC\n",
      "地區 Nc\n",
      "首府 Nc\n",
      "史提帕納科特 PER\n",
      "（ PARENTHESISCATEGORY\n",
      "stepanakert  FW\n",
      "） PARENTHESISCATEGORY\n",
      "分別 D\n",
      "遭 P\n",
      "砲火 Na\n",
      "攻擊 Nv\n",
      "。 PERIODCATEGORY\n"
     ]
    }
   ],
   "source": [
    "output = ''\n",
    "\n",
    "for tok, tag in zip(toks, tags):\n",
    "    print(tok, tag)\n",
    "    output += tok + ' {' + tag + '}' + '   '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
